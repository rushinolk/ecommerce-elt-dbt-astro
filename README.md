# Olist Data Pipeline: Engenharia de Dados com dbt, Airflow (Cosmos) e AWS

## ğŸ“– Sobre o Projeto

Este projeto implementa um pipeline de dados **ELT (Extract, Load, Transform)** completo utilizando o dataset do E-commerce Olist. O objetivo foi simular um ambiente corporativo moderno, onde a infraestrutura Ã© gerenciada via cÃ³digo e a qualidade dos dados Ã© garantida atravÃ©s de testes automatizados.

O projeto utiliza o **Astro CLI** para gerenciamento do ambiente Airflow e segue uma arquitetura modular: uma DAG dedicada para ingestÃ£o de dados brutos (Python) e outra para transformaÃ§Ã£o (dbt), garantindo desacoplamento e facilidade de manutenÃ§Ã£o.
---

## ğŸ—ï¸ Arquitetura (Medallion)

O pipeline foi desenhado seguindo a arquitetura **Medallion**, orquestrada em duas etapas distintas:

1.  **Ingestion Layer (DAG `01_ingestion`):**
    * **Setup Database (`setup_database`):** Task inicial que utiliza `PostgresHook` para garantir a limpeza e criaÃ§Ã£o dos schemas (`bronze_olist`, etc.) antes da carga. Isso garante **idempotÃªncia**: o pipeline limpa o prÃ³prio ambiente antes de comeÃ§ar.
    * Scripts Python extraem os dados brutos (CSV).
    * Converte para o formato parquet e carga na camada **Bronze** no **AWS RDS (PostgreSQL)**.
    * **Trigger Controller:** Ao finalizar o sucesso da carga, utiliza o `TriggerDagRunOperator` para disparar automaticamente a prÃ³xima etapa.

3.  **Transformation Layer (DAG `02_transform`):**
    * Disparada automaticamente apÃ³s o sucesso da ingestÃ£o (Dataset/Trigger).
    * O **dbt Core** assume o comando para transformar os dados dentro do banco (ELT).
    * **Silver:** Limpeza (`COALESCE`), tipagem (`CAST`) e padronizaÃ§Ã£o.
    * **Gold:** Modelagem dimensional (Fato/DimensÃµes) e deduplicaÃ§Ã£o de clientes.

---

## ğŸ› ï¸ Tech Stack

* **Linguagem:** Python 3.9+ & SQL
* **OrquestraÃ§Ã£o & Infra:** Apache Airflow via **Astro CLI**
* **TransformaÃ§Ã£o:** dbt Core (integrado via Cosmos/BashOperator)
* **Banco de Dados:** AWS RDS (PostgreSQL)
* **Gerenciamento de Config:** `airflow_settings.yaml` (Connections as Code)
---

## ğŸ“ Estrutura do Projeto

```text
olist-data-pipeline/
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ 01_ingestion.py       # ExtraÃ§Ã£o e Carga (Python Puro)
â”‚   â””â”€â”€ 02_transform.py       # TransformaÃ§Ã£o (dbt runner)
â”œâ”€â”€ include/
â”‚   â””â”€â”€ dbt/                  # Projeto dbt completo
â”‚       â”œâ”€â”€ data/             # Dados brutos (CSV)
â”‚       â”œâ”€â”€ models/           # Modelos de transformaÃ§Ã£o (DBT)
â”‚       â”œâ”€â”€ data_staging/     # Dados convertidos (PARQUET)
â”‚       â”œâ”€â”€ tests/            # Testes singulares
â”‚       â””â”€â”€ dbt_project.yml
â”œâ”€â”€ tests/                    # Testes unitÃ¡rios do Airflow
â”œâ”€â”€ airflow_settings.yaml     # ConfiguraÃ§Ã£o automÃ¡tica de conexÃµes
â”œâ”€â”€ Dockerfile                # CustomizaÃ§Ã£o da imagem Astro Runtime
â””â”€â”€ README.md
```

---
## âœ¨ Destaques TÃ©cnicos

### 1. Pipeline Idempotente e Auto-GerenciÃ¡vel
A task `setup_database` na DAG de ingestÃ£o roda comandos DDL (`DROP SCHEMA IF EXISTS` / `CREATE SCHEMA`) antes de qualquer dado ser processado. Isso torna o pipeline **resiliente**: ele garante um estado limpo a cada execuÃ§Ã£o, evitando conflitos ou duplicidade de dados antigos na camada Bronze.

### 2. Arquitetura Desacoplada (IngestÃ£o vs TransformaÃ§Ã£o)
Ao invÃ©s de uma DAG monolÃ­tica, separei as responsabilidades. A DAG `01_ingestion.py` foca apenas em extrair e carregar o dado bruto. Ao finalizar, ela aciona a `02_transform.py`. Isso facilita o *backfill* e a manutenÃ§Ã£o: se a regra de negÃ³cio muda, rodo apenas a transformaÃ§Ã£o, sem precisar reprocessar a ingestÃ£o (API/CSV).

### 3. Data Quality e Testes (dbt)
A confianÃ§a no dado Ã© garantida via `schema.yml`. O pipeline falha automaticamente se:
* **Integridade:** Um pedido na tabela fato referenciar um cliente inexistente (`relationships`).
* **Completude:** IDs crÃ­ticos estiverem nulos (`not_null`).
* **LÃ³gica de NegÃ³cio:** Tratamento de categorias nulas (`COALESCE`) e conversÃ£o de datas (`CAST`) direto no SQL.

### 4. Gerenciamento de ConexÃµes (IaC)
Eliminei a necessidade de configurar conexÃµes manualmente na interface do Airflow a cada deploy. Utilizei o arquivo `airflow_settings.yaml` para definir as credenciais do **AWS RDS** como cÃ³digo, garantindo que o ambiente suba pronto para uso.

### 5. DeduplicaÃ§Ã£o AvanÃ§ada (SQL)
ImplementaÃ§Ã£o de lÃ³gica **SCD Tipo 1** na camada Gold para unificar clientes duplicados, utilizando Window Functions (`ROW_NUMBER`) para priorizar sempre o registro mais recente do cliente.

```sql
/* Exemplo da lÃ³gica de deduplicaÃ§Ã£o */
ROW_NUMBER() OVER(
    PARTITION BY customer_unique_id 
    ORDER BY customer_id DESC
) as rn
... WHERE rn = 1
```

---

## ğŸš€ Como Executar Localmente

### PrÃ©-requisitos
* Docker Desktop instalado e rodando.
* **Astro CLI** instalado (Ferramenta de linha de comando da Astronomer).

### Passo a Passo

1.  **Clone o repositÃ³rio:**
    ```bash
    git clone [https://github.com/rushinolk/ecommerce-elt-dbt-astro.git](https://github.com/rushinolk/ecommerce-elt-dbt-astro.git)
    cd olist-data-pipeline
    ```

2.  **Verifique as ConexÃµes:**
    O arquivo `airflow_settings.yaml` jÃ¡ estÃ¡ configurado para criar a conexÃ£o `postgres_olist_dw` automaticamente.
    *(Certifique-se de que suas credenciais da AWS ou do Banco Local estejam corretas neste arquivo).*



3.  **Inicie o Ambiente Astro:**
    Este comando irÃ¡ construir a imagem Docker e subir os containers do Airflow (Webserver, Scheduler, Triggerer e Postgres de metadados).
    ```bash
    astro dev start
    ```

4.  **Acesse o Airflow:**
    Abra `http://localhost:8080` no seu navegador.
    * **UsuÃ¡rio:** `admin`
    * **Senha:** `admin`
    
    Ative a DAG **`01_ingestion`** e acompanhe o fluxo completo atÃ© a transformaÃ§Ã£o no dbt.
    

---

## ğŸ“Š PrÃ³ximos Passos
* [ ] ConstruÃ§Ã£o de Dashboard no Power BI conectado Ã  camada Gold.

---

### ğŸ“¬ Contato
Gostou do projeto? Vamos conectar!
* [LinkedIn](https://www.linkedin.com/in/arthur-gomes1/)
